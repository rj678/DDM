{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an error-correcting DDM workflow\n",
    "\n",
    "This notebook documents the workflow used to identify, analyze and correct the errors associated with using a physically based model (MODFLOW) for hydrological (groundwater/surface-water) simulation.\n",
    "\n",
    "Based on DDM-UQ (Data Driven Model-Uncertainty Quantification) framework specified in Xu and Valocchi, 2015.\n",
    "\n",
    "This notebook uses the Python version of the MATLAB based DDM-UQ toolbox.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "### DDM:\n",
    "\n",
    "* https://wiki.illinois.edu/wiki/display/mlgwm/Home\n",
    "\n",
    "* T. Xu, A. J. Valocchi, 2015. Data-driven Methods to Improve Baseflow Prediction of a Regional Groundwater Model. Computers and Geociences. doi: 10.1016/j.cageo.2015.05.016\n",
    "\n",
    "* T. Xu, A. J. Valocchi, J. Choi, E. Amir, 2013. Use of Machine Learning Methods to Reduce Predictive Error of Groundwater Models. Groundwater. doi: 10.1111/gwat.12061\n",
    "\n",
    "\n",
    "## Contact author:\n",
    "\n",
    "Rishi Jumani; unbiased.modeler@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main concepts\n",
    "\n",
    "\n",
    "## Account for three main types of errors\n",
    "\n",
    "* Structural errors\n",
    "\n",
    "* Paramter errors\n",
    "\n",
    "* Measurement errors\n",
    "\n",
    "## Advantages of using DDM:\n",
    "\n",
    "* Works for errors from multiple sources, and does not make any assumptions about the error distribution\n",
    "\n",
    "* May be possible for the DDM to generalize to new hydrogeological conditions, that differ from the training data\n",
    "\n",
    "\n",
    "\n",
    "## Disadvantages of using DDM:\n",
    "\n",
    "* Requires existence of structural errors (spatial and temporal patterns). Might not be suitable for well calibrated models (calibration error follows Gaussian distribution with zero mean and variance similar to observation error). - ** This workflow attempts to address this disadvantage by including more input features than considered in the papers  **\n",
    "\n",
    "* DDM may not conserve mass\n",
    "\n",
    "\n",
    "## Residual Analysis\n",
    "\n",
    "* Learn a relation between the residuals (errors) and the input factors.\n",
    "\n",
    "* Model the spatio-temporal residuals - uncover the bias, correlation, heteroscedasticity in the results, by capturing the systematic patterns in the residuals.\n",
    "\n",
    "\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "### Support Vector Machine (SVM)\n",
    "\n",
    "* Goal: Find a relation between the residual $\\epsilon$ and selected model inputs $\\mathbf{x}$:\n",
    "\n",
    "### $$\\epsilon = \\hat{\\epsilon}(\\mathbf{x})$$\n",
    "\n",
    "* Train the DDM with the help of a test dataset.\n",
    "\n",
    "* Has good generalization performance, and finds the global minima.\n",
    "\n",
    "* Minimize the upper bound of the generalization error rather than minimizing the training error\n",
    "\n",
    "\n",
    "\n",
    "## Error Variance Analysis (EVA)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Structural Errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Further work\n",
    "\n",
    "* Build local DDM (based on clustering the residuals, so that residuals in the same cluster have similar characteristics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freyberg Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration\n",
    "\n",
    "### Calibrated Parameters:\n",
    "\n",
    "* Hydraulic Conductivity\n",
    "\n",
    "\n",
    "### Observations used for Calibration:\n",
    "\n",
    "* Head measurements\n",
    "\n",
    "* Baseflow\n",
    "\n",
    "### Calibration Technique\n",
    "\n",
    "* Pilot Points\n",
    "\n",
    "\n",
    "### Software used\n",
    "\n",
    "pyEMU and PEST++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis\n",
    "\n",
    "* Compute Mean Error (ME) of the head (negative value if simulated head $>$ observed head): if magnitude of ME is large, this would indicate bias in the residuals\n",
    "\n",
    "* Construct a semivariogram to examine the spatial correlation in the residual\n",
    "\n",
    "* Use residual plots to estimate bias and heteroscedasticity\n",
    "\n",
    "* Use Q-Q plot to test for Gaussianity\n",
    "\n",
    "* Use the Durbin-Watson (DW) test to examine the  temporal correlation (auto-correlation) in the residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of semivariogram\n",
    "\n",
    "* If the variance of the difference between the residuals at two wells increases as the lag distance increases within the range, this would indicate the existence of spatial dependence\n",
    "\n",
    "* What is the nugget effect and what is the range? (More prominent nugget effect and smaller range indicate that spatial correlation of error is not as strong)\n",
    "\n",
    "* Is the density of monitoring wells sufficient: if spatial correlation grows weaker as distance increases, then it is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Q plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of DW statistic\n",
    "\n",
    "The DW statistic tests for $1^{st}$ order auto-correlation\n",
    "\n",
    "The DW statistic works with time series data with uniform time intervals: only include wells that have relatively uniformly spaced water level measurements\n",
    "\n",
    "If the DW statistic if a majority of the wells is $<$ 2, this would indicate that the residuals are correlated in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Average Mututal Information scores\n",
    "\n",
    "To check if the historical residuals are related to the well location, simulated head or time of measuremenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  %%javascript\n",
    "#     MathJax.Hub.Config({\n",
    "#        TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "#      });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train the SVM\n",
    "\n",
    "### Software used: scikit-learn\n",
    "\n",
    "### Ways to apply the DDM:\n",
    "\n",
    "* Temporal Prediction during prediction period\n",
    "\n",
    "* Spatial Predicition\n",
    "\n",
    "* Spatial-Temporal Prediction\n",
    "\n",
    "\n",
    "### Basic Theory\n",
    "\n",
    "* Project the input $\\mathbf{x}$ to a higher dimensional feature space with the map $\\phi : \\mathcal{X} \\rightarrow \\mathcal{F}$\n",
    "\n",
    "* Carry out a linear regression in the feature space $\\phi(\\mathbf{x})$:\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\mathbf{w} \\cdot \\phi (\\mathbf{x}) + b $$\n",
    "\n",
    "* estimate the coefficients $\\mathbf{w}$ and b by solving an optimization problem:\n",
    "\n",
    "$$ \\begin{equation} \\label{one} \n",
    "\\mbox{ minimize } \\frac{1}{2} || \\mathbf{w} ||^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "\\end{equation} $$\n",
    "\n",
    "$$ \\begin{equation} \\label{two} \n",
    "\\mbox{ subject to } (\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b) - y_i \\le \\varepsilon + \\xi_i\n",
    "\\end{equation} $$\n",
    "\n",
    "$$\\begin{equation} \\label{three} \n",
    "y_i - \\left( \\mathbf{w}^T \\phi(\\mathbf{x}_i) + b  \\right) \\le \\varepsilon + \\xi_i^*\n",
    "\\end{equation} $$\n",
    "\n",
    "$$ \\begin{equation} \\label{four} \n",
    "\\xi_i, \\xi_i^* \\ge 0, \\mbox{  } i = 1,.., n\n",
    "\\end{equation} $$\n",
    "\n",
    "* The first term in \\eqref{one} represents the complexity of the regression model, and acts as regularization.\n",
    "\n",
    "* The second term in \\eqref{one} represents the goodness-of-fit of the training data\n",
    "\n",
    "* The slack variables $\\xi_i, a\\xi_i^*$ are used to cope with infeasible constraints of the optimization problem, and are derived from the $\\varepsilon$-insensitive loss function $|\\xi|_\\varepsilon = max \\{0, |y_i - f(\\mathbf{x}_i| - \\varepsilon \\} $\n",
    "\n",
    "* The constant C, known as the regularization hyper-parameter, in \\eqref{one} determines the trade-off between the flatness of f and the deviations exceeding $\\varepsilon$\n",
    "\n",
    "* The map $\\phi : \\mathcal{X} \\rightarrow \\mathcal{F}$ is implemented via kernel functions:\n",
    "\n",
    "$$ \\langle \\phi (\\mathbf{x}_i) , \\phi (\\mathbf{x}_j) \\rangle = K(\\mathbf{x}_i, \\mathbf{x}_j) $$\n",
    "\n",
    "* Use an RBF (Radial Basis Function) kernel:\n",
    "\n",
    "$$ K(\\mathbf{x}_i, \\mathbf{x}_j) = exp(-\\gamma \\mbox{  } || \\mathbf{x}_i - \\mathbf{x}_j ||^2)   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose input data\n",
    "\n",
    "* For temporal and spatio-temporal prediction scenarios, that require the extrapolation of time, including time as an input feature leads to inferior performance of the DDM, while for **spatial prediction**, including time as an input feature yields better performance.\n",
    "\n",
    "#### Input features for temporal and spatio-temporal predictions:\n",
    "\n",
    "* Well location $(x_w, y_w)$ of observations\n",
    "\n",
    "* Simulated head, $\\hat{h}$\n",
    "\n",
    "\n",
    "#### Input features for spatial predictions:\n",
    "\n",
    "* In addition to the above two factors, also includes time\n",
    "\n",
    "\n",
    "#### Input parameters to consider:\n",
    "\n",
    "* Pumping rates\n",
    "\n",
    "* Boundary Conditions\n",
    "\n",
    "* Month\n",
    "\n",
    "* location of observations\n",
    "\n",
    "* Precip rates\n",
    "\n",
    "* ET\n",
    "\n",
    "* difference between groundwater head and streambed elevation\n",
    "\n",
    "* simulated baseflow\n",
    "\n",
    "\n",
    "### Selection criteria\n",
    "\n",
    "* Add input parameters one by one, and accept a parameter if CV error decreases after adding it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data into Training and Testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the parameters\n",
    "\n",
    "* Use k-fold cross validation (CV); k chosen as 5, 10\n",
    "\n",
    "* Data was partitioned differently for spatial and spatio-temporal scenarios, and for temporal scenario\n",
    "\n",
    "### Parameters to be tuned:\n",
    "\n",
    "* regularization hyper-parameter, C (determines the complexity of the DDM) \n",
    "\n",
    "* loss function parameter $\\varepsilon$\n",
    "\n",
    "* Kernel width parameter $\\gamma$\n",
    "\n",
    "### References\n",
    "\n",
    "* Cherkassky and Ma, 2004\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the DDM\n",
    "\n",
    "* Use the DDM with the tuned parameters, to re-train the DDM with the entire training dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the DDM\n",
    "\n",
    "### Use the trained DDM to correct the MODFLOW generated residuals\n",
    "\n",
    "$$ \\hat{h}^{new} = \\hat{h} + \\hat{\\epsilon}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$ z_i = M_i + y_i = M_i + \\hat{y}(\\mathbf{x}_i , \\phi) + \\varphi_i $$\n",
    "\n",
    "where $z_i$ is the quantity of interest (head), $M_i$ is the simulated equivalent, $y_i$ is the lumped model residual, $\\hat{y}$ is the epistemic term (bias), $\\phi$ represents the hyper-parameters and $\\varphi_i$ is the aleatoric term (independently distributed random noise), whose distribution is estimated using SVM (parametric approach) .\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the distribution of the aleatoric term\n",
    "\n",
    "* Use 10 fold CV once more, and fit a distribution to the CV generalization errors\n",
    "\n",
    "* Distribution type is chosen on a case-by-case basis - Gaussian is the most widely used noise model; Laplace and Cauchy distributions have pdf that might resemble the shape of the histogram of the aleatoric errors. The Cauchy distribution has heavy tails, which would allow including outliers.\n",
    "\n",
    "* All 3 distributions have 2 parameters:\n",
    "\n",
    " - Gaussian and Laplace: mean and variance\n",
    " \n",
    " - Cauchy: median and inter-quartile range\n",
    " \n",
    " * The parameters can be inferred from the CV generalization errors using maximum likelihood estimation (MLE)\n",
    " \n",
    " * The goodness-of-fit can be assessed by comparing the likelihood corresponding to the estimated parameters (choosing the distribution with the highest likelihood)\n",
    " \n",
    " * $F(\\varphi)$ is the cdf of the fitted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the DDM\n",
    "\n",
    "* Use a testing dataset $\\{x_i^*, z_i^*\\}$\n",
    "\n",
    "* We have two outputs: $\\hat{z}_i^* = M_i^* + \\hat{y}_i^*$ and the associated prediction interval $[L_i, U_i]$\n",
    "\n",
    "* 2 ways to test: does the DDM reduce the the predicitve bias of the PBM, and evaluate the quality of the predicition intervals.\n",
    "\n",
    "* The DDM-corrected error is $e_i^* = z_i^* - \\hat{z}_i^*$. Three statistics to evaluate the error are:\n",
    "\n",
    "1). PBIAS (percent bias): \n",
    "\n",
    "$$ PBIAS = \\frac{\\sum_{i=1}^m e_i^*}{\\sum_{i=1}^m z_i^*} \\times 100\\%  $$\n",
    "\n",
    "2). MAE (mean absolute error):\n",
    "\n",
    "$$ MAE = \\frac{1}{m} \\sum_{j=1}^m |e_i^*|  $$\n",
    "\n",
    "3). NSE (Nash-Sutcliffe efficiency) - range of NSE varies from $- \\infty$ and $1.0$:\n",
    "\n",
    "$$ NSE = 1 - \\frac{\\sum_{j=1}^m (e_i^*)^2}{\\sum_{j=1}^m (z_i^* - \\bar{z}_i^*)^2}   $$\n",
    "\n",
    "\n",
    "* The quality of the prediction intervals is evaluated using the prediction interval coverage probability (PICP), defined as the percentage of total observations that fall into the estimated prediction interval:\n",
    "\n",
    "$$ PICP = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1} \\{ L_i \\le z_i^* \\le U_i  \\}     $$\n",
    "\n",
    "where $\\mathbf{1} \\{ L_i \\le z_i^* \\le U_i  \\}  $ is an indicator function that equals $1$ if the observation falls between the interval and $0$ otherwise. A prediction interval with $90\\%$ confidence level should theoretically cover $90\\%$ os observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Prediction interval\n",
    "\n",
    "* Impose the aleatoric error distribution on the DDM-corrected prediction\n",
    "\n",
    "* The interval $[L_i, U_i]$ can be consructed with a specified confidence level $1 - \\alpha$\n",
    "\n",
    "$$ L_i = M_i + y_i^L = M_i + \\hat{y}_i + F^{-1}(\\alpha/2)   $$\n",
    "\n",
    "$$ U_i = M_i + y_i^U = M_i + \\hat{y}_i + F^{-1}(1 - \\alpha/2)   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* Comparison of pre-DDM and post-DDM based on ME, RMSE; residual plots; hydrographs at representative wells\n",
    "\n",
    "* Does the DDM remove the global bias - is the ME $\\approx$ Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
